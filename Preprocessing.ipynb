{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21046, 42)\n(2629, 42)\n(2573, 42)\n"
     ]
    }
   ],
   "source": [
    "training = pd.read_csv(\"./dataset/disaster_response_messages_training.csv\")\n",
    "test = pd.read_csv(\"./dataset/disaster_response_messages_test.csv\")\n",
    "validation = pd.read_csv(\"./dataset/disaster_response_messages_validation.csv\")\n",
    "\n",
    "print(training.shape)\n",
    "print(test.shape)\n",
    "print(validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_cols = ['id', 'message', 'request', 'food', 'shelter', 'water', 'medical_help', 'medical_products', 'clothing', 'search_and_rescue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_clean = pd.DataFrame()\n",
    "test_clean = pd.DataFrame()\n",
    "validation_clean = pd.DataFrame()\n",
    "\n",
    "for col in chosen_cols:\n",
    "    training_clean[col] = training[col]\n",
    "    test_clean[col] = test[col]\n",
    "    validation_clean[col] = validation[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# create a new lemmetizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = filter(lambda x: x.isalpha(), tokens)\n",
    "    tokens = map(lambda x: x.lower(), tokens)\n",
    "    \n",
    "    return list(tokens)\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Returns the sentence with each word lemmatized\n",
    "    tokens = get_tokens(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            result.append(lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB'):\n",
    "            result.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('JJ'):\n",
    "            result.append(lemmatizer.lemmatize(word, pos='a'))\n",
    "        elif tag.startswith('R'):\n",
    "            result.append(lemmatizer.lemmatize(word, pos='r'))\n",
    "        else:\n",
    "            result.append(word)\n",
    "    \n",
    "    return \" \".join(result)\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    # Returns the sentence with each word stemmed\n",
    "    tokens = get_tokens(sentence)\n",
    "    result = []\n",
    "\n",
    "    for tok in tokens:\n",
    "        result.append(porter.stem(tok))\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = training.message.to_list()\n",
    "messages_stemmed = []\n",
    "\n",
    "for message in messages:\n",
    "    messages_stemmed.append(stem_sentence(message))\n",
    "\n",
    "messages_lemmatized = []\n",
    "\n",
    "for message in messages:\n",
    "    messages_lemmatized.append(lemmatize_sentence(message))\n",
    "\n",
    "training_clean['message_stem'] = messages_stemmed\n",
    "training_clean['message_lemma'] = messages_lemmatized\n",
    "\n",
    "messages = test.message.to_list()\n",
    "messages_stemmed = []\n",
    "\n",
    "for message in messages:\n",
    "    messages_stemmed.append(stem_sentence(message))\n",
    "\n",
    "messages_lemmatized = []\n",
    "\n",
    "for message in messages:\n",
    "    messages_lemmatized.append(lemmatize_sentence(message))\n",
    "\n",
    "test_clean['message_stem'] = messages_stemmed\n",
    "test_clean['message_lemma'] = messages_lemmatized\n",
    "\n",
    "messages = validation.message.to_list()\n",
    "messages_stemmed = []\n",
    "\n",
    "for message in messages:\n",
    "    messages_stemmed.append(stem_sentence(message))\n",
    "\n",
    "messages_lemmatized = []\n",
    "\n",
    "for message in messages:\n",
    "    messages_lemmatized.append(lemmatize_sentence(message))\n",
    "\n",
    "validation_clean['message_stem'] = messages_stemmed\n",
    "validation_clean['message_lemma'] = messages_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21046, 12)\n(2629, 12)\n(2573, 12)\n"
     ]
    }
   ],
   "source": [
    "print(training_clean.shape)\n",
    "print(test_clean.shape)\n",
    "print(validation_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_clean.to_csv('./preprocessed/training.csv', index=False)\n",
    "test_clean.to_csv('./preprocessed/test.csv', index=False)\n",
    "validation_clean.to_csv('./preprocessed/validation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}